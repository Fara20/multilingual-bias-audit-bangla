{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multilingual Bias Audit - Model Evaluation\n",
        "## Notebook 01: Run toxicity detection models on EN/BN pairs\n",
        "\n",
        "**Author**: Farahnaz Reza  \n",
        "**Project**: Evaluating language bias in toxicity detection  \n",
        "**Dataset**: 30 paired prompts (English + Bangla)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Mount Drive and Load Dataset"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and load dataset\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/multilingual-bias-audit/data/pairs.csv')\n",
        "\n",
        "# Display basic info\n",
        "print(\"✓ Dataset loaded successfully!\")\n",
        "print(f\"\\nTotal samples: {len(df)}\")\n",
        "print(f\"Languages: {df['language'].value_counts().to_dict()}\")\n",
        "print(f\"Categories: {df['category'].value_counts().to_dict()}\")\n",
        "print(\"\\nFirst 3 pairs:\")\n",
        "print(df.head(6))"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Install Required Libraries"
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install toxicity detection libraries\n",
        "!pip install -q transformers detoxify\n",
        "print(\"✓ Libraries installed successfully!\")"
      ],
      "metadata": {
        "id": "install_libs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Initialize Models"
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from detoxify import Detoxify\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Initializing toxicity detection models...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Model 1: Unitary Toxic-BERT (multilingual)\n",
        "try:\n",
        "    toxic_classifier = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=\"unitary/toxic-bert\",\n",
        "        tokenizer=\"unitary/toxic-bert\"\n",
        "    )\n",
        "    print(\"✓ Loaded: unitary/toxic-bert\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading toxic-bert: {e}\")\n",
        "    toxic_classifier = None\n",
        "\n",
        "# Model 2: Detoxify (multilingual)\n",
        "try:\n",
        "    detoxify_model = Detoxify('multilingual')\n",
        "    print(\"✓ Loaded: Detoxify (multilingual)\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading Detoxify: {e}\")\n",
        "    detoxify_model = None\n",
        "\n",
        "print(\"\\n✓ Model initialization complete!\")"
      ],
      "metadata": {
        "id": "init_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Run Inference on All Texts"
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_with_toxic_bert(text):\n",
        "    \"\"\"Run toxic-bert classification\"\"\"\n",
        "    if toxic_classifier is None:\n",
        "        return None, None\n",
        "    \n",
        "    try:\n",
        "        result = toxic_classifier(text)[0]\n",
        "        label = result['label']\n",
        "        score = result['score']\n",
        "        return label, score\n",
        "    except Exception as e:\n",
        "        print(f\"Error classifying: {text[:50]}... | {e}\")\n",
        "        return None, None\n",
        "\n",
        "def classify_with_detoxify(text):\n",
        "    \"\"\"Run Detoxify classification\"\"\"\n",
        "    if detoxify_model is None:\n",
        "        return None, None\n",
        "    \n",
        "    try:\n",
        "        results = detoxify_model.predict(text)\n",
        "        toxicity_score = results['toxicity']\n",
        "        label = 'toxic' if toxicity_score > 0.5 else 'non-toxic'\n",
        "        return label, toxicity_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error classifying: {text[:50]}... | {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Run both models on all texts\n",
        "print(\"Running inference on all texts...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "results = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    text = row['text']\n",
        "    pair_id = row['pair_id']\n",
        "    language = row['language']\n",
        "    category = row['category']\n",
        "    expected = row['expected_label']\n",
        "    \n",
        "    # Model 1: Toxic-BERT\n",
        "    bert_label, bert_score = classify_with_toxic_bert(text)\n",
        "    \n",
        "    # Model 2: Detoxify\n",
        "    detox_label, detox_score = classify_with_detoxify(text)\n",
        "    \n",
        "    results.append({\n",
        "        'pair_id': pair_id,\n",
        "        'language': language,\n",
        "        'text': text,\n",
        "        'category': category,\n",
        "        'expected_label': expected,\n",
        "        'toxic_bert_label': bert_label,\n",
        "        'toxic_bert_score': bert_score,\n",
        "        'detoxify_label': detox_label,\n",
        "        'detoxify_score': detox_score\n",
        "    })\n",
        "    \n",
        "    print(f\"Processed {idx+1}/{len(df)}: pair {pair_id} ({language})\")\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n✓ Inference complete!\")\n",
        "print(f\"\\nResults preview:\")\n",
        "print(results_df[['pair_id', 'language', 'category', 'detoxify_label', 'detoxify_score']].head(10))"
      ],
      "metadata": {
        "id": "run_inference"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Quick Analysis - Language Bias Detection"
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"BIAS ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def analyze_language_bias(results_df, model_name, label_col, score_col):\n",
        "    print(f\"\\n--- {model_name} ---\")\n",
        "    \n",
        "    # Overall accuracy by language\n",
        "    for lang in ['en', 'bn']:\n",
        "        lang_df = results_df[results_df['language'] == lang]\n",
        "        if label_col in lang_df.columns and lang_df[label_col].notna().any():\n",
        "            correct = (lang_df['expected_label'] == lang_df[label_col]).sum()\n",
        "            total = len(lang_df)\n",
        "            accuracy = correct / total if total > 0 else 0\n",
        "            print(f\"{lang.upper()} accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
        "    \n",
        "    # Label agreement between EN and BN for each pair\n",
        "    agreements = 0\n",
        "    total_pairs = results_df['pair_id'].nunique()\n",
        "    \n",
        "    for pair_id in results_df['pair_id'].unique():\n",
        "        pair_df = results_df[results_df['pair_id'] == pair_id]\n",
        "        if len(pair_df) == 2 and label_col in pair_df.columns:\n",
        "            en_label = pair_df[pair_df['language'] == 'en'][label_col].values\n",
        "            bn_label = pair_df[pair_df['language'] == 'bn'][label_col].values\n",
        "            if len(en_label) > 0 and len(bn_label) > 0:\n",
        "                if en_label[0] == bn_label[0]:\n",
        "                    agreements += 1\n",
        "    \n",
        "    agreement_rate = agreements / total_pairs if total_pairs > 0 else 0\n",
        "    print(f\"Label agreement (EN/BN pairs): {agreement_rate:.2%} ({agreements}/{total_pairs})\")\n",
        "\n",
        "# Analyze both models\n",
        "if 'toxic_bert_label' in results_df.columns:\n",
        "    analyze_language_bias(results_df, \"Toxic-BERT\", 'toxic_bert_label', 'toxic_bert_score')\n",
        "\n",
        "if 'detoxify_label' in results_df.columns:\n",
        "    analyze_language_bias(results_df, \"Detoxify\", 'detoxify_label', 'detoxify_score')"
      ],
      "metadata": {
        "id": "quick_analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Identify Harmful Error Patterns"
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CONCERNING CASES (Harmful Errors)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "concerning_cases = []\n",
        "\n",
        "for pair_id in results_df['pair_id'].unique():\n",
        "    pair_df = results_df[results_df['pair_id'] == pair_id]\n",
        "    \n",
        "    if len(pair_df) == 2:\n",
        "        en_row = pair_df[pair_df['language'] == 'en'].iloc[0]\n",
        "        bn_row = pair_df[pair_df['language'] == 'bn'].iloc[0]\n",
        "        \n",
        "        # Using Detoxify results\n",
        "        if 'detoxify_label' in results_df.columns:\n",
        "            en_pred = en_row['detoxify_label']\n",
        "            bn_pred = bn_row['detoxify_label']\n",
        "            expected = en_row['expected_label']\n",
        "            \n",
        "            # Pattern 1: Toxic content marked safe in BN (false negative)\n",
        "            if expected == 'toxic' and en_pred == 'toxic' and bn_pred == 'non-toxic':\n",
        "                concerning_cases.append({\n",
        "                    'pair_id': pair_id,\n",
        "                    'pattern': 'False Negative in BN',\n",
        "                    'description': 'Toxic content detected in EN but missed in BN',\n",
        "                    'en_text': en_row['text'][:60],\n",
        "                    'bn_text': bn_row['text'][:60],\n",
        "                    'harm': 'Users unprotected from harassment'\n",
        "                })\n",
        "            \n",
        "            # Pattern 2: Neutral content flagged in BN (false positive)\n",
        "            if expected == 'neutral' and en_pred == 'non-toxic' and bn_pred == 'toxic':\n",
        "                concerning_cases.append({\n",
        "                    'pair_id': pair_id,\n",
        "                    'pattern': 'False Positive in BN',\n",
        "                    'description': 'Neutral content flagged as toxic only in BN',\n",
        "                    'en_text': en_row['text'][:60],\n",
        "                    'bn_text': bn_row['text'][:60],\n",
        "                    'harm': 'Legitimate speech censored'\n",
        "                })\n",
        "\n",
        "if concerning_cases:\n",
        "    print(f\"\\n⚠️ Found {len(concerning_cases)} concerning patterns:\\n\")\n",
        "    for case in concerning_cases:\n",
        "        print(f\"Pair {case['pair_id']}: {case['pattern']}\")\n",
        "        print(f\"  Impact: {case['harm']}\")\n",
        "        print(f\"  EN: {case['en_text']}...\")\n",
        "        print(f\"  BN: {case['bn_text']}...\\n\")\n",
        "else:\n",
        "    print(\"\\n✓ No major concerning patterns detected in this batch.\")\n",
        "    print(\"(Expand dataset to 30 pairs for fuller analysis)\")"
      ],
      "metadata": {
        "id": "harmful_errors"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Save Results to Drive"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results back to your Drive\n",
        "output_path = '/content/drive/MyDrive/multilingual-bias-audit/results/'\n",
        "\n",
        "# Create results folder if it doesn't exist\n",
        "import os\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Save predictions\n",
        "results_df.to_csv(output_path + 'predictions.csv', index=False)\n",
        "print(f\"✓ Results saved to: {output_path}predictions.csv\")\n",
        "\n",
        "# Save summary statistics\n",
        "with open(output_path + 'summary.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(\"MULTILINGUAL BIAS AUDIT - SUMMARY\\n\")\n",
        "    f.write(\"=\"*50 + \"\\n\\n\")\n",
        "    f.write(f\"Total pairs analyzed: {results_df['pair_id'].nunique()}\\n\")\n",
        "    f.write(f\"Total samples: {len(results_df)}\\n\\n\")\n",
        "    \n",
        "    if concerning_cases:\n",
        "        f.write(f\"Concerning patterns found: {len(concerning_cases)}\\n\\n\")\n",
        "        for case in concerning_cases:\n",
        "            f.write(f\"Pair {case['pair_id']}: {case['pattern']}\\n\")\n",
        "            f.write(f\"  {case['description']}\\n\")\n",
        "            f.write(f\"  Impact: {case['harm']}\\n\\n\")\n",
        "\n",
        "print(f\"✓ Summary saved to: {output_path}summary.txt\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Review predictions.csv for detailed model outputs\")\n",
        "print(\"2. Expand dataset to 30 pairs for comprehensive analysis\")\n",
        "print(\"3. Document key findings in your GitHub repository\")\n",
        "print(\"4. Create visualizations of bias patterns\")"
      ],
      "metadata": {
        "id": "save_results"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
